{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCI HUB Downlod\n",
    "\n",
    "#### This script downloads articles directly from SCIHUB informing DOI\n",
    "\n",
    "Adaped by Edson Melo de Souza (2019)<br>\n",
    "Original source: https://github.com/mikesuhan/scihubscraper\n",
    "\n",
    "<ul>\n",
    "    <li>Some articles can be downloaded, but have PDF conversion errors</li>\n",
    "    <li>Check the error log file to fix</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# This import is for use with Bibtex files.\n",
    "import regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n",
    "logging.basicConfig(filename='errors.log', level=logging.ERROR)\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    s = requests.Session()\n",
    "    headers = {\n",
    "            'User-Agent':\n",
    "            'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "        }\n",
    "    html = s.get(url, timeout=60, headers=headers, allow_redirects=False)\n",
    "    html.encoding = 'utf-8'\n",
    "    html.raise_for_status()\n",
    "    html = bs4.BeautifulSoup(html.text, 'html.parser')\n",
    "    return html\n",
    "\n",
    "def get_article(doi, base_url='http://sci-hub.tw/', folder=''):\n",
    "    url = base_url + doi\n",
    "    html = get_html(url)\n",
    "    time.sleep(3)\n",
    "    iframe = html.find('iframe', {'id': 'pdf'})\n",
    "\n",
    "    if iframe is not None:\n",
    "        pdf_url = iframe['src']\n",
    "        if pdf_url.startswith('//'):\n",
    "            pdf_url = 'http:' + pdf_url\n",
    "\n",
    "        pdf = requests.get(pdf_url)\n",
    "        pdf_fn = pdf_url.split('/')[-1].split('.pdf')[0]\n",
    "        new_fn = '{0}_{1}.pdf'.format(pdf_fn, doi.replace('/', ' '))\n",
    "        file_path = os.path.join(folder, new_fn)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(pdf.content)\n",
    "        logger.info(doi)\n",
    "        time.sleep(3)\n",
    "    else:\n",
    "        logger.error(doi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use with Bibitex files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = regex.compile(r'doi = \\{((?:[^{}]|(?R))*)\\}')\n",
    "f = open('my_collection.bib', 'r', encoding=\"utf8\").read()\n",
    "\n",
    "result = r.findall(f)\n",
    "\n",
    "for x in result:\n",
    "\tif x != 'nan':\n",
    "\t\tget_article(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use with DOI arrays to manual scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [ \n",
    "        '10.1186/s13059-018-1602-2',\n",
    "        '10.1186/s13059-018-1611-1',\n",
    "        '10.1186/s13059-018-1607-x',\n",
    "        '10.1186/s13059-018-1599-6',\n",
    "        '10.1186/s13059-018-1603-1',\n",
    "        '10.1186/s13059-018-1600-4',\n",
    "        '10.1186/s13059-018-1601-3',\n",
    "        '10.1016/j.cub.2017.11.007',\n",
    "        '10.1016/j.cub.2017.11.027',\n",
    "        '10.1016/j.cub.2017.11.031',\n",
    "        '10.1016/j.cub.2017.11.039',\n",
    "        '10.1016/j.cub.2017.11.063',\n",
    "        '10.1016/j.cub.2017.11.048',\n",
    "        '10.1016/j.cub.2017.11.021',\n",
    "        '10.1016/j.annepidem.2010.08.004',\n",
    "        '10.1007/s11192-013-1084-7',\n",
    "        '10.1007/s11192-019-03086-z',\n",
    "        '10.1007/s11192-015-1683-6',\n",
    "        '10.1007/s00223-018-0492-3'\n",
    "    ]\n",
    "try:\n",
    "    for x in array:\n",
    "        get_article(x)\n",
    "        \n",
    "except MyError as e:\n",
    "    print('Erro:', e.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
